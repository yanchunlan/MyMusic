01. FFmpeg获取视频原始AVPacket
    通过av_read_frame获取AVMEDIA_TYPE_VIDEO类型的视频帧，存储到
    视频队列（queue）中。

02. FFmpeg获取视频AVFrame
        步骤一：
            AVPacket *avPacket = av_packet_alloc();
            queue->getAvpacket(avPacket);
            avcodec_send_packet(avCodecContext, avPacket);
        步骤二：
            AVFrame *avFrame = av_frame_alloc();
            avcodec_receive_frame(avCodecContext, avFrame);

    释放video资源:
        1、释放队列
        	delete(queue);

        2、释放解码器上下文
        avcodec_close(avCodecContext);
        avcodec_free_context(&avCodecContext);

03.获取YUV数据，并用OpenGLES渲染数据
    什么是YUV格式？
        YUV，是一种颜色编码方法。Y表示明亮度，也就是灰度值。U和V则是色度、
        浓度，作用是描述影像色彩及饱和度，用于指定像素的颜色。

        主要用于电视系统以及模拟视频领域，它将亮度信息（Y）与色彩信息（UV）
        分离，没有UV信息一样可以显示完整的图像，显示出来将是黑白效果。

    什么是YUV420？
        YUV420是指：Y : UV = 4 : 1
    什么是YUV420P？
        YUV420P是指：YUV的排列方式，先将Y排列完，再将U排列完，最后将
        V排列完。如：
        YYYYYYYYYYYYYYYY UUUU VVVV

    FFmpeg中YUV数据：
        FFmpeg解码出来的视频YUV数据是存储在AVFrame中的data里面，我们以
        YUV420P为视频数据给OPenGL渲染。

        Y分量：frame->data[0]
        U分量：frame->data[1]
        V分量：frame->data[2]

        绝大多数视频都是YUV420P格式的，对于不是YUV420P格式的，我们先将
        其转换（sws_scale）为YUV420P后再给OPenGL渲染。

    OpenGL ES渲染YUV420P：
        为什么用OpenGL来处理YUVP颜色格式视频？

        OpenGL中是不能直接渲染YUV数据的，但是我们可以用3个纹理来
        分别获取Y、U和V的值，然后根据公式：

        r = y + 1.403 * v;
        g = y - 0.344 * u - 0.714 * v;
        b = y + 1.770 * u;

        转为rgb颜色格式显示出来。这个转换过程是在GPU中完成的，计算
        效率比在CPU中计算高很多倍！

    Shader：

        vertex_shader.glsl：

            attribute vec4 av_Position;
            attribute vec2 af_Position;
            varying vec2 v_texPosition;
            void main() {
                v_texPosition = af_Position;
                gl_Position = av_Position;
            }

            注： attribute 只能在vertex中使用，varying 用于vertex和fragment之间传递值

        fragment_shader.glsl：

            precision mediump float;
            varying vec2 v_texPosition;
            uniform sampler2D sampler_y;
            uniform sampler2D sampler_u;
            uniform sampler2D sampler_v;
            void main() {
                float y,u,v;
                y = texture2D(sampler_y,v_texPosition).r;
                u = texture2D(sampler_u,v_texPosition).r- 0.5;
                v = texture2D(sampler_v,v_texPosition).r- 0.5;

                vec3 rgb;
                rgb.r = y + 1.403 * v;
                rgb.g = y - 0.344 * u - 0.714 * v;
                rgb.b = y + 1.770 * u;

                gl_FragColor = vec4(rgb,1);
            }
            注： uniform 用于在application中向vertex和fragment中传递值。

04.	音视频同步

	1.什么是音视频同步？
		由于视频播放器中音频和视频是分别播放和渲染的，就会出现声音和画面不同步
		的现象。为了使同一时刻声音和画面的一致性，我们就需要音视频同步来实现，
		这就是音视频同步。
	2.音频播放时间和视频播放时间：
		音频播放：
			音频播放的时长是PCM数据决定的，根据数据大小和采样率、通道数和位深
			度就能计算出播放的时长。只要采样率、通道数、位深度不变，扬声器播放
			同一段PCM数据的时间就是固定不变的。
		视频播放：
			视频其实没有播放时长的概念，只有相邻视频画面帧直接的时间间隔，调整
			时间间隔就能改变视频画面的渲染速度，来实现视频的快慢控制。
	3.音视频同步方法：
		第一种：音频线性播放，视频同步到音频上。
		第二种：视频线性播放，音频同步到视频上。
		第三种：用一个外部线性时间，音频和视频都同步到这个外部时间上。
		由于人们对声音更敏感，视频画面的一会儿快一会儿慢是察觉不出来的。而声音的节奏变
		化是很容易察觉的。所以我们这里采用第一种方式来同步音视频。
	4.音视频同步实现：
		1、PTS和time_base
			PTS即显示时间戳，这个时间戳用来告诉播放器该在什么时候显示这一帧的数据。
			time_base即时间度量单位（时间基），可以类比：米、千克这种单位。
		2、分别获取音频和视频的PTS（播放时间戳）：
			PTS = avFrame->pts * av_q2d(avStream->time_base);
		3、获取音视频PTS差值，根据差值来设置视频的睡眠时间达到和音频的相对同步。
			视频快了就休眠久点，视频慢了就休眠少点，来达到同步。

05.	视频seek功能

	1、Seek函数：
		avformat_seek_file(pFormatCtx, -1, INT64_MIN, relsecds, INT64_MAX, 0);
		relsecds单位： int64_t
	2、Seek后还需要清楚音频和视频的buffer：
		avcodec_flush_buffers(avCodecContext);
		注：此时需要给avCodecContext添加线程锁，不然avcodec_send_packet
		和avcodec_receive_frame也会操作avCodecContext，导致崩溃。

06.	MediaCodec硬解码

	一.	检测是否可以被硬解码：
		1、解码流程：
			根据FFmpeg中视频解码器的名称找到对应手机硬解码器，如果存在则可以硬解码，
			走硬解码流程；不存在就只能走软解码流程。
		2、硬解码：
			使用MediaCodec直接解码AVpacket，此时需要对AVPacket进行格式过滤，然后
			MediaCodec解码后的数据用OpenGL ES渲染出来。
		3、软解码：
			直接用OpenGL ES 渲染YUV数据。
			
		检测过程：
			1、FFmpeg视频解码器名称获取
				((const AVCodec*)(video->avCodecContext->codec))->name;
			2、测试找出FFmpeg视频解码器对应的硬解码器名称，如：
				"h264“  "video/avc“
			3、遍历手机解码器查找是否存在：
				MediaCodecList
				
	二.	AVPacket添加解码头信息
	
		FFmpeg解码获得的AVPacket只包含视频压缩数据，并没有包含相关的解码信息
		（比如：h264的sps pps头信息，AAC的adts头信息），没有这些编码头信息解
		码器（MediaCodec）是识别不到不能解码的。在FFmpeg中，这些头信息是保存
		在解码器上下文（AVCodecContext）的extradata中的，所以我们需要为每一种
		格式的视频添加相应的解码头信息，这样解码器（MediaCodec）才能正确解析
		每一个AVPacket里的视频数据。
		
		AVBitStreamFilter:
			旧版：av_bitstream_filter_filter
				可以实现头信息添加，但是容易造成内存泄漏，处理比较麻烦
				给AVPacket添加指定解码格式的头信息。
			新版：AVBitStreamFilter
				使用简单，没有内存泄漏问题（测试过）。
		添加过程:
			1、找到相应解码器的过滤器
				bsfilter = av_bsf_get_by_name("h264_mp4toannexb");
			2、初始化过滤器上下文：
				av_bsf_alloc(bsfilter, &bsf_ctx); //AVBSFContext;
			3、添加解码器属性：
				avcodec_parameters_copy(bsf_ctx->par_in, video->codecpar); 
			4、初始化过滤器上下文
				av_bsf_init(video->bsf_ctx)；
			5、送入AVPacket过滤
				av_bsf_send_packet(bsf_ctx, avPacket)；
			6、接收过滤后的AVPacket：
				av_bsf_receive_packet(bsf_ctx, avPacket);
			7、释放资源：
				av_bsf_free(&bsf_ctx);
				
	三.	MediaCodec解码AVpacket
	
		1、初始化MediaCodec：
		
			解码器类型（mime），视频宽度（width），视频高度（height），最大数据
			输入大小(max_input_size)，csd-0，csd-1。

			mediaFormat = MediaFormat.createVideoFormat(mime, width, height);
			mediaFormat.setInteger(MediaFormat.KEY_WIDTH, width);
			mediaFormat.setInteger(MediaFormat.KEY_HEIGHT, height);
			mediaFormat.setLong(MediaFormat.KEY_MAX_INPUT_SIZE, width * height);
			mediaFormat.setByteBuffer(“csd-0”, ByteBuffer.wrap(csd0)); //avCodecContext->extradata
			mediaFormat.setByteBuffer("csd-1", ByteBuffer.wrap(csd1)); //avCodecContext->extradata

		2、MediaCodec开始解码：
		
			int inputBufferIndex = mediaCodec.dequeueInputBuffer(10);
			if(inputBufferIndex >= 0)
			 {
				ByteBuffer byteBuffer = mediaCodec.getInputBuffers()[inputBufferIndex];
				byteBuffer.clear();
				byteBuffer.put(bytes);
				mediaCodec.queueInputBuffer(inputBufferIndex, 0, size, pts, 0);
			}
				int index = mediaCodec.dequeueOutputBuffer(info, 10);
				while (index >= 0) {
				mediaCodec.releaseOutputBuffer(index, true);
				index = mediaCodec.dequeueOutputBuffer(info, 10);
			}
			size:avPacket->size， bytes :avPacket->data
			
	四.	OpenGL渲染MediaCodec解码数据
	
			1、OpenGL生成纹理
			2、纹理绑定到SurfaceTexture上
			3、用SurfaceTexture做参数创建Surface
			4、MediaCodec解码的视频就往Surface发送，就显示出画面了

		vertex_shader.glsl:
		
			attribute vec4 av_Position;
			attribute vec2 af_Position;
			varying vec2 v_texPo;
			void main() {
				v_texPo = af_Position;
				gl_Position = av_Position;
			}

			注： attribute 只能在vertex中使用
					varying 用于vertex和fragment之间传递值
					
		fragment_mediacodec.glsl:

			#extension GL_OES_EGL_image_external : require
			precision mediump float;
			varying vec2 v_texPosition;
			uniform samplerExternalOES sTexture;

			void main() {
				gl_FragColor=texture2D(sTexture, v_texPosition);
			}

			注： uniform 用于在application中向vertex和fragment中传递值。

07.	优化

	C++线程退出：
		单个线程退出：
			使用return 代替 pthread_exit();
		多线程退出：
			使用pthread_join(thread_t, NULL)，会阻塞当前线程，直到thread_t退出完。